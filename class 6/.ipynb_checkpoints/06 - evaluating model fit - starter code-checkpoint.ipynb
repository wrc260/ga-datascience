{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Evaluating Model Fit\n",
    "## Starter code for guided practice & demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn import cross_validation, linear_model, metrics\n",
    "%matplotlib inline\n",
    "\n",
    "# Config\n",
    "sns.set_style(\"darkgrid\")\n",
    "DATA_DIR = Path('.')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path('../datasets')\n",
    "print \"DATA_DIR:\", DATA_DIR\n",
    "print \"CSV filepath:\", DATA_DIR / 'msleep.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"Let's compare two random models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create two dfs of data\n",
    "df = pd.DataFrame({'x': range(100), 'y': range(100)})\n",
    "biased_df = df.copy()\n",
    "biased_df.loc[:20, 'x'] = 1\n",
    "biased_df.loc[:20, 'y'] = 1\n",
    "\n",
    "# What do these look like?\n",
    "print df.head()\n",
    "print\n",
    "print biased_df.head()\n",
    "print sns.lmplot('x', 'y', df, fit_reg=False)\n",
    "print sns.lmplot('x', 'y', biased_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now \"jitter\" them by adding some random variates\n",
    "def append_jitter(series):\n",
    "    jitter = 10 * np.random.random_sample(size=100)\n",
    "    return series + jitter\n",
    "\n",
    "df['x'] = append_jitter(df.x)\n",
    "df['y'] = append_jitter(df.y)\n",
    "\n",
    "biased_df['x'] = append_jitter(biased_df.x)\n",
    "biased_df['y'] = append_jitter(biased_df.y)\n",
    "\n",
    "print df.head()\n",
    "print\n",
    "print biased_df.head()\n",
    "print sns.lmplot('x', 'y', df, fit_reg=False)\n",
    "print sns.lmplot('x', 'y', biased_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Fit a LinearRegression to each\n",
    "lm = linear_model.LinearRegression().fit(df[['x']], df['y'])\n",
    "biased_lm = linear_model.LinearRegression().fit(biased_df[['x']], biased_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate MSE for unbiased model (trained on unbiased data) against the unbiased data\n",
    "print metrics.mean_squared_error(df['y'], lm.predict(df[['x']]))\n",
    "\n",
    "# Evaluate the MSE for biased model (trained on biased data) against the BIASED data\n",
    "print metrics.mean_squared_error(biased_df['y'], biased_lm.predict(biased_df[['x']]))\n",
    "\n",
    "# Evaluate the MSE for biased model (trained on biased data) against the UNBIASED data\n",
    "print metrics.mean_squared_error(df['y'], biased_lm.predict(df[['x']]))\n",
    "\n",
    "# If model (or dataset on which it was trained) has a lot of bias, it may have low MSE\n",
    "# or good R^2 on its own dataset, but won't generalise as well to new data. We'd therefore\n",
    "# like to find a way to prevent against this bias (this will unfortunately increase variance,\n",
    "# i.e. R^2 will decrease & MSE will increase for the unbiased model vs unbiased data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"Using k-fold cross-validation with MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in bikeshare data\n",
    "bikeshare = pd.read_csv(DATA_DIR / 'bikeshare.csv')\n",
    "bikeshare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy the weather categories & create new modelling dataset + output vector from 'casual' column\n",
    "weather = pd.get_dummies(bikeshare.weathersit, prefix='weather')\n",
    "modeldata = bikeshare[['temp', 'hum']].join(weather[['weather_1', 'weather_2', 'weather_3']])\n",
    "y = bikeshare.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create k-fold cross-validation (kf contains indices for selecting train/test sets)\n",
    "kf = cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True)\n",
    "print kf\n",
    "list(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build models on subsets of the data\n",
    "scores = []\n",
    "for train_index, test_index in kf:\n",
    "    train_X = modeldata.iloc[train_index]\n",
    "    train_y = y.iloc[train_index]\n",
    "    test_X = modeldata.iloc[test_index]\n",
    "    test_y = y.iloc[test_index]\n",
    "    lm = linear_model.LinearRegression().fit(train_X, train_y)\n",
    "    test_predictions = lm.predict(test_X)\n",
    "    scores.append(metrics.mean_squared_error(test_y, test_predictions))\n",
    "    \n",
    "print scores\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This score will be lower (i.e. better), we're reducing variance by improving bias (using all the data)\n",
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "print \"MSE for simple linear regression:\", metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "\n",
    "# Which approach do you think would predict more accurately for new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Cross-validation for linear regression\n",
    "\n",
    "If we were to continue increasing the number of folds in cross-validation, would error increase or decrease?\n",
    "\n",
    "Directions:\n",
    "1. Using the above code example, perform k-fold cross validation for all even numbers between 2 and 50. (Hint:  range(2, 51, 2) produces a list of even numbers from 2 to 50)\n",
    "2. Answer the following questions:\n",
    "  - What does `shuffle=True` do?\n",
    "  - At what point does cross validation no longer seem to help the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"Where regularisation makes sense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate MSE for whole model using LinearRegression (no regularisation), then regularised regression\n",
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "lasso_lm = linear_model.Lasso().fit(modeldata, y)\n",
    "ridge_lm = linear_model.Ridge().fit(modeldata, y)\n",
    "\n",
    "# These scores will be worse (i.e. higher error) - we're now trading off bias error for generalised error\n",
    "print \"MSE for simple linear regression:\", metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print \"MSE for Lasso regression:\", metrics.mean_squared_error(y, lasso_lm.predict(modeldata))\n",
    "print \"MSE for Ridge regression:\", metrics.mean_squared_error(y, ridge_lm.predict(modeldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Understanding regularisation effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's test a variety of alpha weights for Ridge Regression on the bikeshare data\n",
    "alphas = np.logspace(start=-10, stop=10, num=21)\n",
    "# Returns numbers between start & stop spaced evenly on a log scale\n",
    "# http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.logspace.html\n",
    "# More: http://stackoverflow.com/questions/31480033/difference-in-output-between-numpy-linspace-and-numpy-logspace\n",
    "print alphas  # See what the logspace() function outputs\n",
    "print sns.distplot(alphas, hist=True, kde=False, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each alpha, fit Ridge regression model to data, print MSE\n",
    "for a in alphas:\n",
    "    print 'Alpha:', a\n",
    "    lm = linear_model.Ridge(alpha=a)\n",
    "    lm.fit(modeldata, y)\n",
    "    print lm.coef_\n",
    "    print metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"We can make this easier with grid search!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "alphas = np.logspace(-10, 10, 21)\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.Ridge(),\n",
    "    param_grid={'alpha': alphas},\n",
    "    scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "print \"Best score\"\n",
    "print -gs.best_score_     # Mean Squared Error here comes in negative, so let's make it positive\n",
    "print\n",
    "print \"Best estimator\"\n",
    "print gs.best_estimator_  # Explains which grid_search setup worked best\n",
    "print\n",
    "print \"All grid pairings & performances\"\n",
    "print gs.grid_scores_     # Shows all the grid pairings and their performances\n",
    "\n",
    "# N.B. MSE lower than above as we're doing 3-fold CV by default!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Grid search & cross-validation\n",
    "Directions:\n",
    "1. Modify the previous code to do the following:\n",
    "  - Introduce cross validation into the grid search.  This is accessible from the cv argument.\n",
    "  - Add fit_intercept = True and False to the param_grid dictionary.\n",
    "\n",
    "2. Re-investigate the best score, best estimator, and grid score attributes as a result of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"Gradient Descent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple example of gradient descent\n",
    "# Problem: Get as close as you can to a given number by taking small pre-defined steps\n",
    "num_to_approach = 6.2\n",
    "start = 0.0\n",
    "steps = [-1, 0.1, 1]\n",
    "optimised = False\n",
    "max_iterations = 100\n",
    "iteration = 0\n",
    "while not optimised and iteration < max_iterations:\n",
    "    current_distance = num_to_approach - start  # initially this would be 6.2\n",
    "    got_better = False  # just initialising the var for now...\n",
    "    next_steps = [start + i for i in steps]  # e.g. from 0.0 this creates [-1, 1]\n",
    "    for n in next_steps:\n",
    "        distance = np.abs(num_to_approach - n)  # e.g. from [-1, 1] this creates [7.2, 5.2]\n",
    "        if distance < current_distance:  # if we're now a little closer...\n",
    "            got_better = True\n",
    "            current_distance = distance\n",
    "            start = n\n",
    "    if got_better:\n",
    "        print 'Iteration {0}: Found better solution using {1} (distance = {2})'.format(iteration, start, current_distance)\n",
    "        iteration += 1\n",
    "    else:\n",
    "        optimized = True\n",
    "        print \"\\nDone! {0} is closest to {1} ({2} iterations)\".format(start, num_to_approach, iteration)\n",
    "        break\n",
    "\n",
    "# See how it performs using different step sizes, e.g. [-0.3, 0.5, 1]\n",
    "# Try changing num_to_approach to np.pi and playing around with steps, random start points, max iterations, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: \"Application of gradient descent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can easily run a gradient descent regression\n",
    "lm = linear_model.SGDRegressor()\n",
    "lm.fit(modeldata, y)\n",
    "print \"R-Squared:\", lm.score(modeldata, y)\n",
    "print \"MSE:\", metrics.mean_squared_error(y, lm.predict(modeldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Activity: \"On your own\"\n",
    "\n",
    "Use the following code to work through the problems given. This code shows a few of the challenges and some of the gotchas of using these techniques. The plots will help showcase some of these.\n",
    "\n",
    "_\"There are tons of ways to approach a regression problem.\"_\n",
    "\n",
    "Directions:\n",
    "1. Implement the Gradient Descent approach to our bikeshare modelling problem.\n",
    "2. Show how Gradient Descent solves and optimises the solution.\n",
    "3. Demonstrate the grid_search module.\n",
    "4. Use a model you evaluated last class or the simpler one from today. Implement param_grid in grid search to answer the following questions:\n",
    "  - With a set of alpha values between 10^-10 and 10^-1, how does the mean squared error change?\n",
    "  - Our data suggests we use L1 regularisation.  By using a grid search with l1_ratios between 0 and 1 (increasing every 0.05), does this statement hold true? If it doesn't look like it does, do we know if gradient descent had enough iterations to work properly? \n",
    "  - How do these results change when you alter the learning rate (power_t)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Starter code\n",
    "params = {} # put your gradient descent parameters here\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "print 'BEST ESTIMATOR'\n",
    "print -gs.best_score_\n",
    "print gs.best_estimator_\n",
    "print 'ALL ESTIMATORS'\n",
    "print gs.grid_scores_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
