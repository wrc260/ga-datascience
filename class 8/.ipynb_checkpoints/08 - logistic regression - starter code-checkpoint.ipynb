{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: Logistic Regression\n",
    "## Starter code for guided practice & demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path('.')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Wager those odds!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Practice: Logit Function and Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logit_func(odds):\n",
    "    # uses a float (odds) and returns back the log odds (logit)\n",
    "    return np.log(odds)\n",
    "\n",
    "def sigmoid_func(logit):\n",
    "    # uses a float (logit) and returns back the probability\n",
    "    return 1 / (1+np.exp(-logit))\n",
    "\n",
    "odds_set = [\n",
    "    4./1,   # AlphaGo : Seedol,   4:1\n",
    "    20./1,  # Chelsea : Leicester City,   20:1\n",
    "    1.1/1,  # England : Wales,   1.1:1\n",
    "    7.0/4,  # Brexit : Remain,   7:4\n",
    "    17.0/3  # President Trump : Not President Trump,   3:17\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 20.0, 1.1, 1.75, 5.666666666666667]\n"
     ]
    }
   ],
   "source": [
    "print odds_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38629436112\n",
      "2.99573227355\n",
      "0.0953101798043\n",
      "0.559615787935\n",
      "1.73460105539\n"
     ]
    }
   ],
   "source": [
    "# Print the probability of the (predicted) better team winning in each case above\n",
    "for i in range(len(odds_set)):\n",
    "    print logit_func(odds_set[i])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.952380952381\n",
      "0.52380952381\n",
      "0.636363636364\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# Print the probability of the (predicted) better team winning in each case above\n",
    "for i in range(len(odds_set)):\n",
    "    print sigmoid_func(logit_func(odds_set[i]))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80000000000000004,\n",
       " 0.95238095238095233,\n",
       " 0.52380952380952384,\n",
       " 0.63636363636363635,\n",
       " 0.84999999999999998]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sigmoid_func(logit_func(odds)) for odds in odds_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Logistic regression implementation\"\n",
    "Use the data titanic.csv and the LogisticRegression estimator in sklearn to predict the target variable `survived`.\n",
    "\n",
    "1. What is the bias, or prior probability, of the dataset?\n",
    "2. Build a simple model with one feature and explore the coef_ value.  Does this represent the odds or logit (log odds)?\n",
    "3. Build a more complicated model using multiple features. Interpreting the odds, which features have the most impact on survival? Which features have the least?\n",
    "4. What is the accuracy of your model?\n",
    "\n",
    "N.B. `age` will need some work (since it is missing for a significant portion), and other data cleanup simplifies the data problem a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(DATA_DIR / 'titanic.csv')\n",
    "\n",
    "# Transform male/female to 1/0\n",
    "titanic['is_male'] = titanic.sex.apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "X = titanic[['is_male']]  # try puting other feature(s) in here\n",
    "y = titanic['survived']\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find out how to print out the log-reg coefficients, intercept and mean survival rate\n",
    "# Docs: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the odds for each coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Evaluating logistic regression with alternative metrics\"\n",
    "This Titanic dataset comes from [Kaggle](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "Spend a few minutes determining which data would be most important to use in the prediction problem. You may need to create new features based on the data available. Consider using a feature selection aide in sklearn. For a worst case scenario, identify one or two strong features that would be useful to include in this model.\n",
    "\n",
    "\n",
    "1. Spend 1-2 minutes considering which metric makes the most sense to optimise. Accuracy? FPR or TPR? AUC? Given the \"business problem\" of understanding survival rate aboard the Titanic, why should you use this metric?\n",
    "\n",
    "2. Build a tuned logistic regression model. Be prepared to explain your design (including regularisation), choice of metric, and your chosen feature set in predicting survival using any tools necessary (such as fit charts). Use the starter code to get you going.\n",
    "\n",
    "N.B. If you haven't done it yet, `age` will need some work (since it is missing for a significant portion), and other data cleanup simplifies the data problem a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's some code for fitting a model and creating an ROC\n",
    "lr = LogisticRegression()\n",
    "X = titanic[['is_male']]  # put your other feature(s) in here\n",
    "y = titanic['survived']\n",
    "lr.fit(X, y)\n",
    "\n",
    "predictions = lr.predict(X)\n",
    "probabilities = lr.predict_proba(X)\n",
    "plt.plot(roc_curve(titanic[['survived']], probabilities[:,1])[0],\n",
    "         roc_curve(titanic[['survived']], probabilities[:,1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To understand this a little further, try printing these in turn\n",
    "#titanic[['survived']]\n",
    "#probabilities\n",
    "#probabilities[:,1]\n",
    "roc_curve(titanic[['survived']], probabilities[:,1])\n",
    "#print roc_curve(titanic[['survived']], probabilities[:,1])[0]\n",
    "#print roc_curve(titanic[['survived']], probabilities[:,1])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve above is based on various probability thresholds (for 'is_male' there's only one thing we can vary, hence one point, joined to (0,0) and (1,1)). This will become more clear if you subtitute e.g. age (after you've cleaned it up!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(roc_curve(titanic[['survived']], predictions)[0],\n",
    "         roc_curve(titanic[['survived']], predictions)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart, which does not play with thresholds, shows the one true TPR and FPR point, joined to 0,0 and 1,1.\n",
    "\n",
    "The first chart will be more effective as you compare models and determine where the decision line should exist for the data. The second simplifies the first in case this idea of thresholds is confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n",
    "roc_auc_score(titanic['survived'], lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
