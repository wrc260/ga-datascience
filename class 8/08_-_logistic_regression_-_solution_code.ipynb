{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: Logistic Regression\n",
    "## Solution code for guided practice & demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "# Config\n",
    "DATA_DIR = Path('../datasets')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Wager those odds!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Practice: Logit Function and Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logit_func(odds):\n",
    "    # uses a float (odds) and returns back the log odds (logit)\n",
    "    return np.log(odds)\n",
    "\n",
    "def sigmoid_func(logit):\n",
    "    # uses a float (logit) and returns back the probability\n",
    "    return 1. / (1 + np.exp(-logit))\n",
    "\n",
    "odds_set = [\n",
    "    4./1,   # AlphaGo : Seedol,   4:1\n",
    "    20./1,  # Chelsea : Leicester City,   20:1\n",
    "    1.1/1,  # England : Wales,   1.1:1\n",
    "    7.0/4,  # Brexit : Remain,   7:4\n",
    "    17.0/3  # President Trump : Not President Trump,   3:17\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.952380952381\n",
      "0.52380952381\n",
      "0.636363636364\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# Print the probability of the (predicted) better team winning in each case above\n",
    "for odds in odds_set:\n",
    "    print sigmoid_func(logit_func(odds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80000000000000004,\n",
       " 0.95238095238095233,\n",
       " 0.52380952380952384,\n",
       " 0.63636363636363635,\n",
       " 0.84999999999999998]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because some of you were asking, here it is as a list comprehension!\n",
    "# Sidenote Python fun...try changing [] to () and seeing what it prints...\n",
    "# What's a \"generator\" in Python? When might these be useful?\n",
    "[sigmoid_func(logit_func(odds)) for odds in odds_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Logistic regression implementation\"\n",
    "Use the data titanic.csv and the LogisticRegression estimator in sklearn to predict the target variable `survived`.\n",
    "\n",
    "1. What is the bias, or prior probability, of the dataset?\n",
    "2. Build a simple model with one feature and explore the coef_ value.  Does this represent the odds or logit (log odds)?\n",
    "3. Build a more complicated model using multiple features. Interpreting the odds, which features have the most impact on survival? Which features have the least?\n",
    "4. What is the accuracy of your model?\n",
    "\n",
    "N.B. `age` will need some work (since it is missing for a significant portion), and other data cleanup simplifies the data problem a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>is_male</th>\n",
       "      <th>had_parents</th>\n",
       "      <th>had_siblings</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      sex   age  sibsp  parch            ticket     fare cabin embarked  \\\n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C   \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S   \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S   \n",
       "\n",
       "   is_male  had_parents  had_siblings  pclass_1  pclass_2  pclass_3  \n",
       "0        1            0             1       0.0       0.0       1.0  \n",
       "1        0            0             1       1.0       0.0       0.0  \n",
       "2        0            0             0       0.0       0.0       1.0  \n",
       "3        0            0             1       1.0       0.0       0.0  \n",
       "4        1            0             0       0.0       0.0       1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(DATA_DIR / 'titanic.csv')\n",
    "\n",
    "# Transform male/female to 1/0\n",
    "titanic['is_male'] = titanic.sex.apply(lambda x: 1 if x == 'male' else 0)\n",
    "\n",
    "# Impute age using mean age for that sex & passenger class\n",
    "titanic['age'] = titanic.groupby([\"sex\", 'pclass']).age.transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Turn parch and sibsp into binary 1/0 predictors\n",
    "titanic['had_parents'] = titanic.parch.apply(lambda x: 1 if x > 0 else 0)\n",
    "titanic['had_siblings'] = titanic.sibsp.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Dummy pclass and join as new cols\n",
    "titanic = titanic.join(pd.get_dummies(titanic.pclass, prefix='pclass'))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3838383838383838"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the inherent bias in the model? (i.e. mean survival rate)\n",
    "titanic.survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's build simple log-reg with one predictor\n",
    "lr = LogisticRegression()\n",
    "X = titanic[['is_male']]\n",
    "y = titanic['survived']\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786756453423\n"
     ]
    }
   ],
   "source": [
    "# Simple accuracy score\n",
    "print lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_male coef = log(odds-ratio)) = [[-2.43010712]]\n",
      "log-reg intercept = [ 1.00027876]\n"
     ]
    }
   ],
   "source": [
    "# Find out how to print out the log-reg coefficients & intercept\n",
    "# Docs: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "print \"is_male coef = log(odds-ratio)) =\", lr.coef_    # this coef is a log(odds-ratio), so a little meaningless!\n",
    "print \"log-reg intercept =\", lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp(is_male coef) = odds-ratio (men vs women) = [[ 0.0880274]]\n",
      "odds-ratio (women vs men) = [[ 11.3600989]]\n"
     ]
    }
   ],
   "source": [
    "# Above suggests that log(OR) of survival for male vs female is -2.43\n",
    "# Therefore odds-ratio of survival (male vs female) is np.exp(-2.43) = 0.088\n",
    "print \"exp(is_male coef) = odds-ratio (men vs women) =\", np.exp(lr.coef_)\n",
    "\n",
    "# Flipping this (1. / 0.0880274), this model suggests that odds of survival for female are\n",
    "# a factor of 11.4 higher than odds of survival for men\n",
    "print \"odds-ratio (women vs men) =\", 1./np.exp(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(survived | female) = [ 0.73111338]\n",
      "P(survived | male) = [[ 0.19312543]]\n"
     ]
    }
   ],
   "source": [
    "# You can turn these into probabilities using the sigmoid function\n",
    "# Remember logistic regression formula applies logit function to the probabilities of survival\n",
    "# i.e. logit(p) = alpha + beta * x\n",
    "\n",
    "# In this case, alpha is intercept, beta is coef, x is feature \"is_male\"\n",
    "\n",
    "# So P(survived | female), read as \"probability of survived, given this person is female\" is found by setting x=0\n",
    "# i.e. logit(p) = alpha + beta * 0\n",
    "#               = alpha\n",
    "\n",
    "# Remember then that sigmoid(logit(p)) = p, so P(survived | female) = sigmoid(alpha) = 73%\n",
    "print \"P(survived | female) =\", sigmoid_func(lr.intercept_)\n",
    "\n",
    "# Similarly, by setting x=1, we get probability of survival for men = sigmoid(alpha + beta * 1) = 19%\n",
    "print \"P(survived | male) =\", sigmoid_func(lr.intercept_ + lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92423881]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's going on here? Surely these two probabilities should add up to 100%??\n",
    "sigmoid_func(lr.intercept_) + sigmoid_func(lr.intercept_ + lr.coef_)\n",
    "\n",
    "# In short, yes. But scikit-learn implements a (by default L2) regularised logistic regression.\n",
    "# I'll demonstrate that this is true, but first, let's calculate the odds ratio by hand..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>survived</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_male</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>468</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "survived    0    1\n",
       "is_male           \n",
       "0          81  233\n",
       "1         468  109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handy function for creating the contingency table of survival vs gender\n",
    "pd.crosstab(titanic['is_male'], titanic['survived'], rownames=['is_male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of surviving if male = 0.188908145581\n",
      "Probability of surviving if female = 0.742038216561\n",
      "Odds of surviving if male = 0.232905982906 : 1\n",
      "Odds of surviving if female = 2.87654320988 : 1\n",
      "Odds ratio of surviving (male vs female) = 0.0809673159459\n",
      "Odds ratio of surviving (female vs male) = 12.3506625892\n"
     ]
    }
   ],
   "source": [
    "# By hand...this is exactly the same as what we went through in class together\n",
    "prob_survive_male = 109.0 / (468 + 109)\n",
    "prob_survive_female = 233.0 / (81 + 233)\n",
    "odds_survive_male = prob_survive_male / (1 - prob_survive_male)\n",
    "odds_survive_female = prob_survive_female / (1 - prob_survive_female)\n",
    "odds_ratio_male_female = odds_survive_male / odds_survive_female\n",
    "\n",
    "print \"Probability of surviving if male =\", prob_survive_male  # 18.9% (notice difference already to above)\n",
    "print \"Probability of surviving if female =\", prob_survive_female  # 74.2% (ditto)\n",
    "print \"Odds of surviving if male = {} : 1\".format(odds_survive_male)  # 0.23:1, or approx 3:13 if you like integers\n",
    "print \"Odds of surviving if female = {} : 1\".format(odds_survive_female)  # 2.88:1, or approx 26:9\n",
    "print \"Odds ratio of surviving (male vs female) =\", odds_ratio_male_female  # 0.08\n",
    "print \"Odds ratio of surviving (female vs male) =\", 1 / odds_ratio_male_female  # 12.4\n",
    "\n",
    "# In English: \"Women were roughly twelve times as likely as men to survive the Titanic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 odds ratio = [[ 11.81280411]]\n",
      "L2 odds ratio = [[ 11.3600989]]\n",
      "L2 + weaker regularisation strength = [[ 12.34723703]]\n"
     ]
    }
   ],
   "source": [
    "# So why the differences? I claimed regularisation...\n",
    "\n",
    "# By way of example, let's just try changing between L1 and L2...\n",
    "print \"L1 odds ratio =\", 1. / np.exp(LogisticRegression(penalty='l1').fit(X, y).coef_)\n",
    "print \"L2 odds ratio =\", 1. / np.exp(LogisticRegression(penalty='l2').fit(X, y).coef_)\n",
    "\n",
    "# And now let's turn regularisation off by weakening the \"regularisation strength\" parameter C...\n",
    "print \"L2 + weaker regularisation strength =\", 1. / np.exp(LogisticRegression(penalty='l1', C=10000).fit(X, y).coef_)\n",
    "\n",
    "# Finally, note that each time you re-run this, the numbers will change. Under the hood, it's using an optimisation\n",
    "# algorithm with a random initial starting point to fit the regularised logistic regression! So no small wonder the\n",
    "# results for odds ratio are coming out as slightly different to the hand calculated odds!\n",
    "\n",
    "# For more detail on scikit-learn's LogisticRegression object:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# For more detail on the relationship between log-reg coefs and hand-calculated ORs (inc. for multiclass case):\n",
    "# http://stats.stackexchange.com/questions/35013/exponentiated-logistic-regression-coefficient-different-than-odds-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using gender as only feature =  0.786756453423\n",
      "Accuracy using age as only feature =  0.616161616162\n",
      "Accuracy using all features =  0.802469135802\n"
     ]
    }
   ],
   "source": [
    "# How does accuracy change depending on features?\n",
    "X = titanic[['is_male']]\n",
    "print \"Accuracy using gender as only feature = \", LogisticRegression().fit(X, y).score(X, y)\n",
    "\n",
    "# Using age (imputed for some individuals)\n",
    "X = titanic[['age']]\n",
    "print \"Accuracy using age as only feature = \", LogisticRegression().fit(X, y).score(X, y)\n",
    "\n",
    "# Using lots of features\n",
    "X = titanic[['is_male', 'age', 'pclass_1', 'pclass_2', 'had_parents', 'had_siblings', 'fare']]\n",
    "print \"Accuracy using all features = \", LogisticRegression().fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "1. Being male decreases the odds of survival.\n",
    "2. Women were roughly twelve times more likely than men to survive the Titanic.\n",
    "3. A model built using only gender has 78.7% accuracy.\n",
    "4. A model built using many features has 80.2% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide: \"Evaluating logistic regression with alternative metrics\"\n",
    "This Titanic dataset comes from [Kaggle](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "Spend a few minutes determining which data would be most important to use in the prediction problem. You may need to create new features based on the data available. Consider using a feature selection aide in sklearn. For a worst case scenario, identify one or two strong features that would be useful to include in this model.\n",
    "\n",
    "\n",
    "1. Spend 1-2 minutes considering which metric makes the most sense to optimise. Accuracy? FPR or TPR? AUC? Given the \"business problem\" of understanding survival rate aboard the Titanic, why should you use this metric?\n",
    "\n",
    "2. Build a tuned logistic regression model. Be prepared to explain your design (including regularisation), choice of metric, and your chosen feature set in predicting survival using any tools necessary (such as fit charts). Use the starter code to get you going.\n",
    "\n",
    "N.B. If you haven't done it yet, `age` will need some work (since it is missing for a significant portion), and other data cleanup simplifies the data problem a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's some code for fitting a model and creating an ROC\n",
    "lr = LogisticRegression()\n",
    "X = titanic[['is_male']]  # put your other feature(s) in here\n",
    "y = titanic['survived']\n",
    "lr.fit(X, y)\n",
    "\n",
    "predictions = lr.predict(X)\n",
    "probabilities = lr.predict_proba(X)\n",
    "plt.plot(roc_curve(titanic[['survived']], probabilities[:,1])[0],\n",
    "         roc_curve(titanic[['survived']], probabilities[:,1])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To understand this a little further, try printing these in turn\n",
    "#titanic[['survived']]\n",
    "#probabilities\n",
    "#probabilities[:,1]\n",
    "roc_curve(titanic[['survived']], probabilities[:,1])\n",
    "#print roc_curve(titanic[['survived']], probabilities[:,1])[0]\n",
    "#print roc_curve(titanic[['survived']], probabilities[:,1])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve above is based on various probability thresholds (for 'is_male' there's only one thing we can vary, hence one point, joined to (0,0) and (1,1)). This will become more clear if you subtitute e.g. age (after you've cleaned it up!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(roc_curve(titanic[['survived']], predictions)[0],\n",
    "         roc_curve(titanic[['survived']], predictions)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This chart, which does not play with thresholds, shows the one true TPR and FPR point, joined to 0,0 and 1,1.\n",
    "\n",
    "The first chart will be more effective as you compare models and determine where the decision line should exist for the data. The second simplifies the first in case this idea of thresholds is confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n",
    "roc_auc_score(titanic['survived'], lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a tuned logistic regression model. Be prepared to explain your design (including regularisation),\n",
    "# choice of metric, and your chosen feature set in predicting survival using any tools necessary (such\n",
    "# as fit charts). Use the starter code to get you going.\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
