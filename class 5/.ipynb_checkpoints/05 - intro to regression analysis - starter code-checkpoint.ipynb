{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Intro To Regression Analysis\n",
    "## Starter code for guided practice & demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# New dataviz library!\n",
    "import seaborn as sns  # may need to `conda install seaborn` or `pip install seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot matplotlib plots & set the seaborn plotting style/size\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to use pathlib for relative paths to our datasets folder (this is a built-in library in Python3)\n",
    "# If running this line throws an error, may need to do: `pip install pathlib` from CLI\n",
    "# For more info see: https://pypi.python.org/pypi/pathlib/\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path('.')                            # You can hardcode a data dir here, e.g. \"~/Dropbox/GA/data/\"\n",
    "print \"DATA_DIR:\", DATA_DIR\n",
    "print \"CSV filepath:\", DATA_DIR / 'msleep.csv'  # pathlib resolves this to OS-relative path to '../../datasets/DAT-8/msleep.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Regression and normal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the mammal dataset\n",
    "mammals = pd.read_csv(str(DATA_DIR / 'msleep.csv'))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows where `brainwt` is missing\n",
    "mammals_where_brainwt_missing = mammals.brainwt.notnull()\n",
    "mammals = mammals[mammals_where_brainwt_missing].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore our mammals dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check out a scatter plot of body weight and brain weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a matplotlib figure\n",
    "plt.figure()\n",
    "# Generate a scatterplot inside the figure\n",
    "plt.plot(mammals.bodywt, mammals.brainwt, '.')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do it using the Seaborn library!\n",
    "# https://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.lmplot.html\n",
    "sns.lmplot('bodywt', 'brainwt', mammals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "1. The lmplot() function returns a straight line. That is why it is a linear solution. If we had multiple variables, the solution would be a linear plane.\n",
    "2. The linear solution does explain a portion of the data well, but because both \"bodywt\" and \"brainwt\" are log-log distributions, outliers have a large impact on the gradient of the linear solution. We can see this from the wide and inconsistently shaped confidence intervals that seaborn's lmplot generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bodywt and brainwt look like they may not be normally distributed, let's check that\n",
    "fig, axs = plt.subplots(2,1)\n",
    "sns.distplot(mammals['bodywt'], color='red', ax=axs[0])\n",
    "sns.distplot(mammals['brainwt'], color='green', ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because both variables are a log-log distribution, some properties of the underlying maths allow us to transform them into normal distributions. Then, we can solve for the linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try log-transforming bodywt and brainwt\n",
    "log_columns = ['bodywt', 'brainwt']\n",
    "log_mammals = mammals.copy()\n",
    "log_mammals[log_columns] = log_mammals[log_columns].apply(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This looks better\n",
    "fig, axs = plt.subplots(2,1)\n",
    "sns.distplot(log_mammals['bodywt'], color='red', ax=axs[0])\n",
    "sns.distplot(log_mammals['brainwt'], color='green', ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the log-transformed scatter\n",
    "sns.lmplot(x='bodywt', y='brainwt', data=log_mammals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Even though we changed the way the data was shaped, this is still a linear result: it's just linear in the log10 of the data, instead of in the data's natural state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Using seaborn to generate single variable linear model plots\n",
    "Update and complete the code below to use lmplot and display correlations between body weight and two dependent variables: sleep_rem and awake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_columns = ['bodywt', 'brainwt',]  # any others?\n",
    "log_mammals = mammals.copy()\n",
    "log_mammals[log_columns] = log_mammals[log_columns].apply(np.log10)\n",
    "\n",
    "# Add code here to create two graphs displaying correlations between sleep_rem and awake (as y) and body weight (as x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Single Regression Analysis in statsmodels & scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the standard import if you're using \"formula notation\" (similar to R)\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "X = mammals[['bodywt']]\n",
    "y = mammals['brainwt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note difference between mammals[['bodywt']] (returns a DataFrame)\n",
    "print type(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ... and mammals['bodywt'] (returns a Series)\n",
    "print type(y)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a fitted model in one line with the following syntax formula:\n",
    "#     outcome ~ predictor1 + predictor2 ... predictorN\n",
    "\n",
    "lm = smf.ols(formula='y ~ X', data=mammals).fit()\n",
    "\n",
    "# Print the full linear model summary\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Statsmodels to make the prediction for a mammal with bodyweight = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You have to create a DataFrame since the Statsmodels formula interface expects it\n",
    "X_new = pd.DataFrame({'X': [50]})\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat in scikit-learn with handy plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When modelling with sklearn, you'll use the following basic principles:\n",
    "\n",
    "- All sklearn estimators (modelling classes) are follow the same \"base estimator\" design. This allows you to easily rotate through estimators without changing much code.\n",
    "- All estimators take a matrix X (can be either sparse or dense).\n",
    "- Many estimators also take a vector of outcome labels, y, when working on a supervised machine learning problem. Regressions are supervised learning because we already have examples of our outcome variable y for every row in X.\n",
    "- All estimators have parameters that can be set. This allows for customization and higher level of detail to the learning process. The parameters are unique to each estimator algorithm.\n",
    "- Finally, note the convention of capital letters for a matrix (e.g. X) vs lower case for a vector (e.g. y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Online documentation is very handy! http://scikit-learn.org/stable/modules/linear_model.html\n",
    "from sklearn import feature_selection, linear_model\n",
    "\n",
    "# Create matrix of features (only one used for now) + vector of outcome labels\n",
    "X = mammals[['bodywt']]\n",
    "y = mammals['brainwt']\n",
    "\n",
    "# Create instance of LinearRegression class and fit model to the data\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# Ignore warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lm is just a \"LinearRegression\" object\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coefs\n",
    "print \"lm.coef_\"\n",
    "print lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# residuals\n",
    "print \"residuals\"\n",
    "print (y - lm.predict(X)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's wrap this into a function that's model-independent\n",
    "\n",
    "def get_linear_model_metrics(X, y, algo):\n",
    "    \"\"\"\n",
    "    Get the p-value of X given y, ignore f-stat for now\n",
    "    \"\"\"\n",
    "    pvals = feature_selection.f_regression(X, y)[1]\n",
    "    # Start with an empty linear regression object\n",
    "    # .fit() runs the linear regression function on X and y\n",
    "    algo.fit(X,y)\n",
    "    residuals = (y - algo.predict(X)).values\n",
    "\n",
    "    # Print the necessary values\n",
    "    print 'P Values:', pvals\n",
    "    print 'Coefficients:', algo.coef_\n",
    "    print 'y-intercept:', algo.intercept_\n",
    "    print 'R-Squared:', algo.score(X,y)\n",
    "\n",
    "    # Visualise distribution of the residuals\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=np.ceil(np.sqrt(len(y))))\n",
    "    plt.title(\"Distribution of residuals\")\n",
    "    \n",
    "    # Keep the model\n",
    "    return algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try it out by building a linear model representing brainwt ~ bodywt\n",
    "# (remember this model is represented as: `brainwt = bodywt * coefficient + intercept`)\n",
    "X = mammals[['bodywt']]\n",
    "y = mammals['brainwt']\n",
    "\n",
    "# Let's visualise this model too\n",
    "sns.lmplot(x='bodywt', y='brainwt', data=mammals, ci=None)\n",
    "plt.title(\"Linear model on untransformed data: brainwt ~ bodywt\")\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm = get_linear_model_metrics(X, y, lm)  # note how lm object is passed into the function, then returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Significance is Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our output tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Fit, Evaluating Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we know there is a better solution to the model, we should evaluate some other \"common sense\" things first. For example, given this model, what is an animal's brainwt if their bodywt is 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What's the prediction at 0? Does it make sense?\n",
    "print lm.predict([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A mammal with zero body weight should also have zero brain weight!\n",
    "\n",
    "# Let's force the intercept to be zero & retrain the model\n",
    "lm = linear_model.LinearRegression(fit_intercept=False)\n",
    "lm = get_linear_model_metrics(X, y, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What's the prediction at 0 now?\n",
    "print lm.predict([[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrepretation\n",
    "With linear modeling we call this part of the linear assumption. Consider it a test to the model. If an animal's body weights nothing, we expect their brain to be nonexistent. That given, we can improve the model by telling sklearn's LinearRegression object we do not want to fit a y intercept.\n",
    "\n",
    "Now, the model fits where brainwt = 0, bodywt = 0.\n",
    "Because we start at 0, the large outliers have a greater effect, so the coefficient has increased.\n",
    "Fitting with this assumption also results in a model that explains slightly less of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Using the LinearRegression object\n",
    "\n",
    "We learned earlier that the data in its current state does not allow for the best linear regression fit. \n",
    "\n",
    "With a partner, generate two more models using the log-transformed data to see how this transform changes the model's performance.\n",
    "\n",
    "Complete the following code to update X and y to match the log-transformed data. Complete the loop by setting the list to be one True and one False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter\n",
    "X =\n",
    "y =\n",
    "loop = []\n",
    "for boolean in loop:\n",
    "    print 'y-intercept:', boolean\n",
    "    lm = linear_model.LinearRegression(fit_intercept=boolean)\n",
    "    get_linear_model_metrics(X, y, lm)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model performed the best? The worst? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more information on how scikit-learn's LinearRegression calculates R-squared, see:\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score\n",
    "> \n",
    "> _\"Best possible R^2 score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Advanced linear regression techniques\n",
    "We will go over different estimators in detail in the future but check it out in the docs if you're curious (and finish a little early)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading other sklearn regression estimators\n",
    "X = log_mammals[['bodywt']]\n",
    "y = log_mammals['brainwt']\n",
    "\n",
    "# Docs = http://scikit-learn.org/stable/modules/linear_model.html\n",
    "estimators = [\n",
    "    linear_model.Lasso(),\n",
    "    linear_model.Ridge(),\n",
    "    linear_model.ElasticNet(),\n",
    "]\n",
    "\n",
    "for est in estimators:\n",
    "    print est\n",
    "    get_linear_model_metrics(X, y, est)\n",
    "    print \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Multiple regression analysis with citibike data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, one variable explained the variance of another; however, more often than not, we will need multiple variables. \n",
    "\n",
    "For example, a house's price may be best measured by square feet, but a lot of other variables play a vital role: bedrooms, bathrooms, location, appliances, etc. \n",
    "\n",
    "For a linear regression, we want these variables to be largely independent of each other, but all of them should help explain the y variable.\n",
    "\n",
    "We'll work with bikeshare data to showcase what this means and to explain a concept called multicollinearity.\n",
    "\n",
    "For more details on this dataset, including a data dictionary, please see: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_data = pd.read_csv(str(DATA_DIR / 'bikeshare.csv'))\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is multicollinearity?\n",
    "\n",
    "With the bike share data, let's compare three data points: actual temperature (**`temp`**), \"feel\" temperature (**`atemp`**), and guest ridership (**`casual`**).\n",
    "\n",
    "Our data is already normalized between 0 and 1, so we'll start off with the correlations and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colour_map = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "correlations = bike_data[['temp', 'atemp', 'casual']].corr()\n",
    "print correlations\n",
    "print sns.heatmap(correlations, cmap=colour_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The correlation matrix explains that:\n",
    "\n",
    "- both temperature fields are somewhat correlated to guest ridership (corr approx 0.5 isn't high, but not low either)\n",
    "- the two temperature fields are highly correlated to each other.\n",
    "\n",
    "\n",
    "Including both of these fields in a model could introduce a pain point of multicollinearity, where it's more difficult for a model to determine which feature is affecting the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can measure this effect in the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = bike_data['casual']\n",
    "x_sets = (\n",
    "    ['temp'],\n",
    "    ['atemp'],\n",
    "    ['temp', 'atemp'],\n",
    ")\n",
    "\n",
    "for x in x_sets:\n",
    "    print 'Model: casual ~ ' + ' + '.join(x)\n",
    "    get_linear_model_metrics(bike_data[x], y, linear_model.LinearRegression())\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Intrepretation: \n",
    "Even though the 2-variable model (**`casual ~ temp + atemp`**) has a higher explanation of variance than two variables on their own, and both variables are considered significant (p values approaching 0), we can see that together, their coefficients are wildly different. \n",
    "\n",
    "This can introduce error in how we explain models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if we use a second variable that isn't highly correlated with temperature, like humidity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sns.heatmap(bike_data[['temp', 'hum', 'casual']].corr(),\n",
    "                  cmap=colour_map)\n",
    "\n",
    "y = bike_data['casual']\n",
    "x = bike_data[['temp', 'hum']]\n",
    "get_linear_model_metrics(x, y, linear_model.LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While temperature's coefficient is higher, the logical output still makes sense: for guest riders we expected a positive relationship with temperature and a negative relationship with humidity, and our model suggests it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Multicollinearity with dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a similar effect from a feature set that is a singular matrix. This happens is when there is a clear relationship in the matrix (for example, the sum of all rows = 1).\n",
    "\n",
    "### Run through the following code on your own.\n",
    "#### What happens to the coefficients when you include all weather situations instead of just including all except one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "# weathersit : \n",
    "# - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "# - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "# - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "# - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "weather = pd.get_dummies(bike_data.weathersit)\n",
    "print weather.head()\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model casual by all weather situations as dummy variables\n",
    "lm = linear_model.LinearRegression()\n",
    "get_linear_model_metrics(weather[[1, 2, 3, 4]], y, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the least significant: weather situation  = 4\n",
    "get_linear_model_metrics(weather[[1, 2, 3]], y, lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar in Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All dummies in the model\n",
    "lm_stats = smf.ols(formula='y ~ weather[[1, 2, 3, 4]]', data=bike_data).fit()\n",
    "lm_stats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop one\n",
    "lm_stats = smf.ols(formula='y ~ weather[[1, 2, 3]]', data=bike_data).fit()\n",
    "lm_stats.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the interpretation ? Do you want to keep all your dummy variables or drop one? Why? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Practice: Combining non-correlated features into a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a partner, complete this code together and visualize the correlations of all the numerical features built into the data set.\n",
    "\n",
    "We want to:\n",
    "\n",
    "- Add the three significant weather situations into our current model.\n",
    "- Find two more features that are not correlated with current features, but could be strong indicators for predicting guest riders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter \n",
    "lm = linear_model.LinearRegression()\n",
    "bikemodel_data = bike_data.join()  # add in the three weather situations here\n",
    "\n",
    "colour_map = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "correlations =  # what are we getting the correlations of?\n",
    "print correlations\n",
    "print sns.heatmap(correlations, cmap=colour_map)\n",
    "\n",
    "columns_to_keep = []  #[which variables?]\n",
    "final_feature_set = bikemodel_data[columns_to_keep]\n",
    "\n",
    "get_linear_model_metrics(final_feature_set, y, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using sklearn + log-transformed y\n",
    "final_feature_set = bikemodel_data[columns_to_keep]\n",
    "get_linear_model_metrics(final_feature_set, np.log10(y + 1), lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Statsmodels + log-transformed y\n",
    "log_y = np.log10(y + 1)\n",
    "lm = smf.ols(formula='log_y ~ temp + hum + windspeed + weather_1 + weather_2 + weather_3 + holiday + hour_1 + hour_2 + hour_3 + hour_4 + hour_5 + hour_6 + hour_7 + hour_8 + hour_9 + hour_10 + hour_11 + hour_12 + hour_13 + hour_14 + hour_15 + hour_16 + hour_18 + hour_19 + hour_20 + hour_21 + hour_22 + hour_23', data=bikemodel_data).fit()\n",
    "# Print the full summary\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Practice / Homework: Building models for other y variables\n",
    "\n",
    "We've completed a model together that explains casual guest riders. Now it's your turn to build another model, using a different y variable: registered riders.\n",
    "\n",
    "#### Pay attention to:\n",
    "\n",
    "* the distribution of riders (should we rescale the data?)\n",
    "* checking correlations with variables and registered riders\n",
    "* having a feature space (our matrix) with low multicollinearity\n",
    "* model complexity vs explanation of variance: at what point do features in a model stop improving r-squared?\n",
    "* the linear assumption -- given all feature values being 0, should we have no ridership? negative ridership? positive ridership?\n",
    "\n",
    "#### Bonus\n",
    "\n",
    "* Which variables would make sense to dummy (because they are categorical, not continuous)?\n",
    "* What features might explain ridership but aren't included in the data set?\n",
    "* Is there a way to build these using pandas and the features available?\n",
    "* Outcomes: If your model at least improves upon the original model and the explanatory effects (coefficients) make sense, consider this a complete task.\n",
    "\n",
    "### If your model for registered riders has an r-squared above .4, this a relatively effective model for the data available. Kudos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model away!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
